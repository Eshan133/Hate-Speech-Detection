{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN813APSaXKK1l2Z3P3gqSo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eshan133/Hate-Speech-Detection/blob/main/Clip_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Mounting the Drive"
      ],
      "metadata": {
        "id": "Mlxt4K0Qux0e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilXzHLE47HBZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate torchvision ftfy regex\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vaGroe2v7KYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from torch.optim import AdamW\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ],
      "metadata": {
        "id": "fJZHR0-Q7SpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. Dataset Preparation"
      ],
      "metadata": {
        "id": "ohGJ6Rabuu-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MemeDataset(Dataset):\n",
        "    def __init__(self, csv_file, processor, is_test=False):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.processor = processor\n",
        "        self.is_test = is_test\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      row = self.data.iloc[idx]\n",
        "      image_path = row['name']\n",
        "      image = Image.open(image_path).convert('RGB')\n",
        "      text = row['text']\n",
        "\n",
        "      item = {\n",
        "          \"image\": image,\n",
        "          \"text\": text\n",
        "      }\n",
        "\n",
        "      if not self.is_test:\n",
        "          label = int(row['label'])\n",
        "          return item, label\n",
        "      else:\n",
        "          return item\n",
        "\n"
      ],
      "metadata": {
        "id": "Pw-sQLyD7Uih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Collate Function"
      ],
      "metadata": {
        "id": "Tlr-C0Ciu90R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Gather texts and images separately\n",
        "    texts = [item[0][\"text\"] for item in batch]\n",
        "    images = [item[0][\"image\"] for item in batch]\n",
        "    labels = torch.tensor([item[1] for item in batch])\n",
        "\n",
        "    # Use processor.__call__ with batching + padding + truncation\n",
        "    encoded_inputs = processor(\n",
        "        text=texts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    return encoded_inputs, labels\n"
      ],
      "metadata": {
        "id": "hHaPuNYb7nws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Dataset Path"
      ],
      "metadata": {
        "id": "vBXxSKc3vXrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv = '/content/drive/MyDrive/Hate Speech Competition/Task_A/train_data.csv'\n",
        "test_csv = '/content/drive/MyDrive/Hate Speech Competition/Task_A/test_data.csv'\n",
        "val_csv = '/content/drive/MyDrive/Hate Speech Competition/Task_A/val_data.csv'"
      ],
      "metadata": {
        "id": "1e3IPgaJ7vMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## 3. Defining Clip Model"
      ],
      "metadata": {
        "id": "nO59YUBdvDaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class MemeCLIPClassifier(nn.Module):\n",
        "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\", num_labels=2):\n",
        "        super(MemeCLIPClassifier, self).__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(model_name)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.clip.config.projection_dim * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, pixel_values, attention_mask):\n",
        "        outputs = self.clip(input_ids=input_ids,\n",
        "                            pixel_values=pixel_values,\n",
        "                            attention_mask=attention_mask,\n",
        "                            return_dict=True)\n",
        "\n",
        "        image_embeds = outputs.image_embeds\n",
        "        text_embeds = outputs.text_embeds\n",
        "        combined = torch.cat((image_embeds, text_embeds), dim=1)\n",
        "\n",
        "        return self.classifier(combined)\n"
      ],
      "metadata": {
        "id": "yHfU4KnD7g_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Freezing layers"
      ],
      "metadata": {
        "id": "oKB5SGVVvOsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_clip_layers(model):\n",
        "    for name, param in model.clip.named_parameters():\n",
        "        if not any(layer in name for layer in [\"visual.transformer.layers.11\", \"text_model.encoder.layers.11\"]):\n",
        "            param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "Epu1iXVO7kGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Initializing"
      ],
      "metadata": {
        "id": "2OUnOUeLvRkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "train_dataset = MemeDataset(train_csv, processor)\n",
        "val_dataset = MemeDataset(val_csv, processor)\n",
        "test_dataset = MemeDataset(test_csv, processor, is_test=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MemeCLIPClassifier()\n",
        "freeze_clip_layers(model)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "sbe-qIQD7qOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4. Training and Evaluation Loop"
      ],
      "metadata": {
        "id": "LRm2-_VevLZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Training Loop"
      ],
      "metadata": {
        "id": "Un67FE2tvdDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for inputs, labels in progress_bar:\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "RVjduV2T9YMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score"
      ],
      "metadata": {
        "id": "6efAcW-fls5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Evaluation Loop"
      ],
      "metadata": {
        "id": "SgLW3NHWvgBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(**inputs)  # logits\n",
        "            probs = torch.softmax(outputs, dim=1)  # shape: [batch_size, 2]\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs[:, 1].cpu().numpy())  # probs for class 1 (hate)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    try:\n",
        "        auc = roc_auc_score(all_labels, all_probs)\n",
        "    except ValueError:\n",
        "        auc = float('nan')\n",
        "\n",
        "    report = classification_report(all_labels, all_preds, target_names=['non-hate', 'hate'])\n",
        "\n",
        "    return acc, f1, auc, report\n"
      ],
      "metadata": {
        "id": "zURoPes7Btca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 5. Training the model"
      ],
      "metadata": {
        "id": "dr4fkjGlvmgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "best_val_f1 = 0\n",
        "patience = 3\n",
        "counter = 0\n",
        "save_path = \"best_model.pt\"\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=1, factor=0.5, verbose=True)\n"
      ],
      "metadata": {
        "id": "unoJvMjLm2Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):  # or any number\n",
        "    print(f\"\\nEpoch: {epoch + 1}\")\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    val_acc, val_f1, val_auc, val_report = evaluate(model, val_loader)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Accuracy: {val_acc:.4f} | F1: {val_f1:.4f} | AUROC: {val_auc:.4f}\")\n",
        "    print(val_report)\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_f1)  # or val_auc or -train_loss\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"✅ Saved new best model with F1: {best_val_f1:.4f}\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"⏳ No improvement. Early stopping counter: {counter}/{patience}\")\n",
        "\n",
        "    if counter >= patience:\n",
        "        print(\"⛔ Early stopping triggered.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "jx02X6149ZIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------"
      ],
      "metadata": {
        "id": "x-7XKRPOjHkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 6. Prediction"
      ],
      "metadata": {
        "id": "a9WUNh9kvrlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def predict_on_test(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, meta in tqdm(test_loader, desc=\"Predicting on test set\"):\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            preds = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
        "\n",
        "            for idx, pred in zip(meta['index'], preds.cpu().numpy()):\n",
        "                predictions.append({\"index\": idx, \"label\": int(pred)})\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "VGh1uCRxoong"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict_on_test(model, test_loader)\n",
        "predictions_sorted = sorted(predictions, key=lambda x: x['index'])"
      ],
      "metadata": {
        "id": "OpcnwMHIotEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 7. Json for submission"
      ],
      "metadata": {
        "id": "mpE3hIUtvxgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"submission.json\", \"w\") as f:\n",
        "    json.dump(predictions_sorted, f, indent=4)\n",
        "\n",
        "!zip -j ref.zip submission.json\n"
      ],
      "metadata": {
        "id": "k9qGNLwaovbu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}