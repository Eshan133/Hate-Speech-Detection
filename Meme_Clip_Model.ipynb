{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxdkgKyF5bDPZEfzvQihpo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eshan133/Hate-Speech-Detection/blob/main/Meme_Clip_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Seo3kXMazPQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Clone the original CLIP repo\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "\n",
        "# Step 2: Install the CLIP repo as a package\n",
        "%cd CLIP\n",
        "!pip install -e .\n"
      ],
      "metadata": {
        "id": "4ZIPbbQsyL_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbaOQrl_v-u-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from clip import clip  # ensure the clip repo or package is available"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "class MemeDataset(Dataset):\n",
        "    def __init__(self, csv_file, processor, is_test=False):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.processor = processor\n",
        "        self.is_test = is_test\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        image_path = row['name']\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        text = row['text']\n",
        "\n",
        "        item = {\n",
        "            \"image\": image,\n",
        "            \"text\": text\n",
        "        }\n",
        "\n",
        "        if not self.is_test:\n",
        "            label = int(row['label'])\n",
        "            return item, label\n",
        "        else:\n",
        "            return item\n"
      ],
      "metadata": {
        "id": "BRphweqKxVkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import CLIPProcessor\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Check if test mode: batch is list of dicts\n",
        "    is_test = isinstance(batch[0], dict)\n",
        "\n",
        "    if is_test:\n",
        "        texts = [item[\"text\"] for item in batch]\n",
        "        images = [item[\"image\"] for item in batch]\n",
        "\n",
        "        encoded_inputs = processor(\n",
        "            text=texts,\n",
        "            images=images,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        return encoded_inputs  # no labels in test\n",
        "    else:\n",
        "        # train/val mode: batch is list of (item, label)\n",
        "        texts = [item[0][\"text\"] for item in batch]\n",
        "        images = [item[0][\"image\"] for item in batch]\n",
        "        labels = torch.tensor([item[1] for item in batch])\n",
        "\n",
        "        encoded_inputs = processor(\n",
        "            text=texts,\n",
        "            images=images,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        return encoded_inputs, labels\n",
        "\n"
      ],
      "metadata": {
        "id": "iUyAR2xsxXfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv = '/content/drive/MyDrive/Hate Speech Competition/Task_A/train_data.csv'\n",
        "test_csv = '/content/drive/MyDrive/Hate Speech Competition/Task_A/test_data.csv'\n",
        "val_csv = '/content/drive/MyDrive/Hate Speech Competition/Task_A/val_data.csv'"
      ],
      "metadata": {
        "id": "i5qoiWg0zdAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "jS3YPoI71mFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MemeDataset(train_csv, processor, is_test=False)\n",
        "val_dataset = MemeDataset(val_csv, processor, is_test=False)\n",
        "test_dataset = MemeDataset(test_csv, processor, is_test=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "uw2bEKAzzIUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineClassifier(nn.Module):\n",
        "    def __init__(self, feat_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(num_classes, feat_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = F.normalize(x, dim=-1)\n",
        "        w_norm = F.normalize(self.weight, dim=-1)\n",
        "        return x_norm @ w_norm.t()"
      ],
      "metadata": {
        "id": "yXzEnN7xwRG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adapter(nn.Module):\n",
        "    def __init__(self, dim, reduction=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim // reduction),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim // reduction, dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x) + x"
      ],
      "metadata": {
        "id": "_BIkCS6IwTjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearProjection(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_layers=1, drop_probs=[0.1, 0.1]):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            in_dim = input_dim if i == 0 else output_dim\n",
        "            layers.append(nn.Linear(in_dim, output_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(drop_probs[min(i, len(drop_probs)-1)]))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "QXpKcy6rwUk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MemeCLIP(nn.Module):\n",
        "    def __init__(self, clip_variant=\"ViT-B/32\", num_classes=2, map_dim=512, num_mapping_layers=1, drop_probs=[0.1, 0.1]):\n",
        "        super().__init__()\n",
        "        self.clip_model, _ = clip.load(clip_variant, device=\"cpu\", jit=False)\n",
        "        self.clip_model.float()\n",
        "\n",
        "        # Freeze CLIP parameters if desired (partial unfreezing can be done later)\n",
        "        for p in self.clip_model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # Projection heads for image and text embeddings\n",
        "        self.image_map = LinearProjection(self.clip_model.visual.output_dim, map_dim, num_mapping_layers, drop_probs)\n",
        "        self.text_map = LinearProjection(self.clip_model.transformer.width, map_dim, num_mapping_layers, drop_probs)\n",
        "\n",
        "        # Adapters for image and text\n",
        "        self.img_adapter = Adapter(map_dim)\n",
        "        self.text_adapter = Adapter(map_dim)\n",
        "\n",
        "        # Cosine similarity classifier\n",
        "        self.classifier = CosineClassifier(map_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values):\n",
        "        # Get CLIP embeddings\n",
        "        image_embeds = self.clip_model.encode_image(pixel_values)\n",
        "        text_embeds = self.clip_model.encode_text(input_ids)\n",
        "\n",
        "        # Project embeddings\n",
        "        image_proj = self.image_map(image_embeds)\n",
        "        text_proj = self.text_map(text_embeds)\n",
        "\n",
        "        # Adapt embeddings\n",
        "        image_feat = self.img_adapter(image_proj)\n",
        "        text_feat = self.text_adapter(text_proj)\n",
        "\n",
        "        # Normalize features\n",
        "        image_feat = F.normalize(image_feat, dim=-1)\n",
        "        text_feat = F.normalize(text_feat, dim=-1)\n",
        "\n",
        "        # Combine features element-wise (Hadamard product)\n",
        "        combined = image_feat * text_feat\n",
        "\n",
        "        # Classify with cosine similarity\n",
        "        logits = self.classifier(combined)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "UcTXwpRYwXD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MemeCLIP(num_classes=2).to(device)  # from previous step\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n"
      ],
      "metadata": {
        "id": "ZD3-Nm6gxUcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        inputs, labels = batch\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(inputs['input_ids'], inputs['attention_mask'], inputs['pixel_values'])\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(dataloader)\n"
      ],
      "metadata": {
        "id": "4BXRyhQFy0yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            inputs, labels = batch\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(inputs['input_ids'], inputs['attention_mask'], inputs['pixel_values'])\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(labels.cpu().tolist())\n",
        "            all_probs.extend(probs[:, 1].cpu().tolist())  # Prob for class 1\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    try:\n",
        "        auc = roc_auc_score(all_labels, all_probs)\n",
        "    except:\n",
        "        auc = 0.0  # fallback for edge cases\n",
        "\n",
        "    return acc, f1, auc\n"
      ],
      "metadata": {
        "id": "dk9gnpHty2Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "best_val_auc = 0.0\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "save_path = \"best_memeclip_model.pt\"\n"
      ],
      "metadata": {
        "id": "lL1TxWLO1UUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20  # or however many you want\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "val_f1s = []\n",
        "val_aucs = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    acc, f1, auc = evaluate(model, val_loader)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val Acc: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
        "\n",
        "    # Append to lists\n",
        "    train_losses.append(train_loss)\n",
        "    val_accuracies.append(acc)\n",
        "    val_f1s.append(f1)\n",
        "    val_aucs.append(auc)\n",
        "\n",
        "    # Step scheduler\n",
        "    scheduler.step(auc)\n",
        "\n",
        "    # Save best model\n",
        "    if auc > best_val_auc:\n",
        "        best_val_auc = auc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(\"✅ Best model saved!\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"⚠️ Patience {patience_counter}/{patience}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= patience:\n",
        "        print(\"⛔ Early stopping triggered.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "R5os0zD9y5ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = list(range(1, len(train_losses) + 1))\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "# Plot 1: Training Loss\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.plot(epochs, train_losses, marker='o')\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "# Plot 2: Validation Accuracy\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.plot(epochs, val_accuracies, marker='o', color='green')\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "# Plot 3: F1 Score\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.plot(epochs, val_f1s, marker='o', color='orange')\n",
        "plt.title(\"Validation F1 Score\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"F1 Score\")\n",
        "\n",
        "# Plot 4: AUC Score\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.plot(epochs, val_aucs, marker='o', color='red')\n",
        "plt.title(\"Validation AUC\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"AUC\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VpRE_n0X_DXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"best_memeclip_model.pt\"))\n",
        "model.to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "xnaVmB2OzpYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "model.eval()\n",
        "submission = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(tqdm(test_loader, desc=\"Inference\")):\n",
        "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        logits = model(inputs['input_ids'], inputs['attention_mask'], inputs['pixel_values'])\n",
        "        preds = torch.argmax(torch.softmax(logits, dim=1), dim=1)\n",
        "        preds = preds.cpu().tolist()\n",
        "\n",
        "        batch_indices = test_dataset.data.iloc[i * test_loader.batch_size : (i+1) * test_loader.batch_size]['index'].tolist()\n",
        "\n",
        "        for idx, label in zip(batch_indices, preds):\n",
        "            submission.append({\n",
        "                \"index\": idx,\n",
        "                \"label\": int(label)\n",
        "            })\n"
      ],
      "metadata": {
        "id": "HlWK3E0B12UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by index\n",
        "submission = sorted(submission, key=lambda x: x[\"index\"])\n",
        "\n",
        "# Save to JSON\n",
        "with open(\"submission.json\", \"w\") as f:\n",
        "    json.dump(submission, f)\n",
        "\n",
        "# Zip it as ref.zip\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"ref.zip\", \"w\") as zipf:\n",
        "    zipf.write(\"submission.json\")\n"
      ],
      "metadata": {
        "id": "4XPrzBLJ13TV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}